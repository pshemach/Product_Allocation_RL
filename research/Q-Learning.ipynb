{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearningAgent(num_products\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_shops\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, max_inventory\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m reward_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Plot training progress\u001b[39;00m\n\u001b[0;32m    106\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(reward_history)\n",
      "Cell \u001b[1;32mIn[3], line 87\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(env, agent, episodes)\u001b[0m\n\u001b[0;32m     85\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[0;32m     86\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m---> 87\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_q_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     89\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[3], line 70\u001b[0m, in \u001b[0;36mQLearningAgent.update_q_table\u001b[1;34m(self, state, action, reward, next_state)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_q_table\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, reward, next_state):\n\u001b[0;32m     69\u001b[0m     product, shop, allocation \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m---> 70\u001b[0m     best_next_q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[product, shop, allocation] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m     72\u001b[0m         reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m best_next_q \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[product, shop, allocation]\n\u001b[0;32m     73\u001b[0m     )\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define Environment\n",
    "class ProductAllocationEnv:\n",
    "    def __init__(self, num_products, num_shops, max_inventory):\n",
    "        self.num_products = num_products\n",
    "        self.num_shops = num_shops\n",
    "        self.max_inventory = max_inventory\n",
    "        self.state = np.zeros((num_products, num_shops))  # Allocation matrix\n",
    "        self.inventory = np.random.randint(\n",
    "            10, max_inventory, size=num_products\n",
    "        )  # Random initial stock\n",
    "        self.demand = np.random.randint(\n",
    "            1, max_inventory // 2, size=(num_products, num_shops)\n",
    "        )  # Random demand\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros((self.num_products, self.num_shops))\n",
    "        self.inventory = np.random.randint(\n",
    "            10, self.max_inventory, size=self.num_products\n",
    "        )\n",
    "        self.demand = np.random.randint(\n",
    "            1, self.max_inventory // 2, size=(self.num_products, self.num_shops)\n",
    "        )\n",
    "        return self.state.flatten()\n",
    "\n",
    "    def step(self, action):\n",
    "        product, shop, allocation = action\n",
    "        allocation = min(allocation, self.inventory[product])\n",
    "        self.inventory[product] -= allocation\n",
    "        self.state[product, shop] += allocation\n",
    "\n",
    "        reward = -abs(self.demand[product, shop] - self.state[product, shop])\n",
    "        done = np.all(self.state >= self.demand) or np.all(self.inventory == 0)\n",
    "        return self.state.flatten(), reward, done\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Inventory:\", self.inventory)\n",
    "        print(\"Demand:\", self.demand)\n",
    "        print(\"Current Allocation:\", self.state)\n",
    "\n",
    "\n",
    "# Q-Learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "        self, num_products, num_shops, max_inventory, alpha=0.1, gamma=0.9, epsilon=0.1\n",
    "    ):\n",
    "        self.q_table = np.zeros((num_products, num_shops, max_inventory + 1))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return (\n",
    "                np.random.randint(0, len(state) // 2),\n",
    "                np.random.randint(0, len(state) // 2),\n",
    "                np.random.randint(1, 10),\n",
    "            )\n",
    "        else:\n",
    "            return np.unravel_index(\n",
    "                np.argmax(self.q_table, axis=None), self.q_table.shape\n",
    "            )\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        product, shop, allocation = action\n",
    "        best_next_q = np.max(self.q_table[next_state])\n",
    "        self.q_table[product, shop, allocation] += self.alpha * (\n",
    "            reward + self.gamma * best_next_q - self.q_table[product, shop, allocation]\n",
    "        )\n",
    "\n",
    "\n",
    "# Train the RL Model\n",
    "def train_agent(env, agent, episodes=1000):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Initialize environment & agent\n",
    "env = ProductAllocationEnv(num_products=5, num_shops=3, max_inventory=50)\n",
    "agent = QLearningAgent(num_products=5, num_shops=3, max_inventory=50)\n",
    "\n",
    "# Train agent\n",
    "reward_history = train_agent(env, agent, episodes=500)\n",
    "\n",
    "# Plot training progress\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Q-Learning Training Progress\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the trained model\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -6181.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 109\u001b[0m\n\u001b[0;32m    106\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearningAgent(num_products\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_shops\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_inventory\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m reward_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Plot training progress\u001b[39;00m\n\u001b[0;32m    112\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(reward_history)\n",
      "Cell \u001b[1;32mIn[4], line 91\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(env, agent, episodes)\u001b[0m\n\u001b[0;32m     88\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 91\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     93\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate_q_table(state, action, reward, next_state)\n",
      "Cell \u001b[1;32mIn[4], line 71\u001b[0m, in \u001b[0;36mQLearningAgent.choose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m     65\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_products),\n\u001b[0;32m     66\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_shops),\n\u001b[0;32m     67\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39munravel_index(\n\u001b[1;32m---> 71\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     72\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\test\\Product_Allocation_RL\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:1342\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1341\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\test\\Product_Allocation_RL\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define Environment\n",
    "class ProductAllocationEnv:\n",
    "    def __init__(self, num_products, num_shops, max_inventory):\n",
    "        self.num_products = num_products\n",
    "        self.num_shops = num_shops\n",
    "        self.max_inventory = max_inventory\n",
    "        self.state = np.zeros((num_products, num_shops))  # Allocation matrix\n",
    "        self.inventory = np.random.randint(\n",
    "            10, max_inventory, size=num_products\n",
    "        )  # Different inventory for each product\n",
    "        self.demand = np.random.randint(\n",
    "            1, max_inventory // 2, size=(num_products, num_shops)\n",
    "        )  # Different demand per shop per product\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros((self.num_products, self.num_shops))\n",
    "        self.inventory = np.random.randint(\n",
    "            10, self.max_inventory, size=self.num_products\n",
    "        )  # Reset inventory per product\n",
    "        self.demand = np.random.randint(\n",
    "            1, self.max_inventory // 2, size=(self.num_products, self.num_shops)\n",
    "        )  # Reset demand per product per shop\n",
    "        return self.state.flatten()\n",
    "\n",
    "    def step(self, action):\n",
    "        product, shop, allocation = action\n",
    "        allocation = min(\n",
    "            allocation,\n",
    "            self.inventory[product],\n",
    "            self.demand[product, shop] - self.state[product, shop],\n",
    "        )\n",
    "        self.inventory[product] -= allocation\n",
    "        self.state[product, shop] += allocation\n",
    "\n",
    "        reward = -abs(self.demand[product, shop] - self.state[product, shop])\n",
    "        done = np.all(self.state >= self.demand) or np.all(self.inventory == 0)\n",
    "        return self.state.flatten(), reward, done\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Inventory:\", self.inventory)\n",
    "        print(\"Demand:\", self.demand)\n",
    "        print(\"Current Allocation:\", self.state)\n",
    "\n",
    "\n",
    "# Q-Learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "        self, num_products, num_shops, max_inventory, alpha=0.1, gamma=0.9, epsilon=0.1\n",
    "    ):\n",
    "        self.num_products = num_products\n",
    "        self.num_shops = num_shops\n",
    "        self.q_table = np.zeros((num_products, num_shops, max_inventory + 1))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return (\n",
    "                np.random.randint(0, self.num_products),\n",
    "                np.random.randint(0, self.num_shops),\n",
    "                np.random.randint(1, 10),\n",
    "            )\n",
    "        else:\n",
    "            return np.unravel_index(\n",
    "                np.argmax(self.q_table, axis=None), self.q_table.shape\n",
    "            )\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        product, shop, allocation = action\n",
    "        best_next_q = np.max(self.q_table)\n",
    "        self.q_table[product, shop, allocation] += self.alpha * (\n",
    "            reward + self.gamma * best_next_q - self.q_table[product, shop, allocation]\n",
    "        )\n",
    "\n",
    "\n",
    "# Train the RL Model\n",
    "def train_agent(env, agent, episodes=1000):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Initialize environment & agent\n",
    "env = ProductAllocationEnv(num_products=10, num_shops=5, max_inventory=100)\n",
    "agent = QLearningAgent(num_products=10, num_shops=5, max_inventory=100)\n",
    "\n",
    "# Train agent\n",
    "reward_history = train_agent(env, agent, episodes=500)\n",
    "\n",
    "# Plot training progress\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Q-Learning Training Progress\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the trained model\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
